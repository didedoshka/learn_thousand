{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFhmE0HxI49t"
   },
   "source": [
    "# Approximate q-learning (10 pts)\n",
    "\n",
    "In this notebook you will teach a __PyTorch__ neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "skMa7z5cI49x"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import thousand\n",
    "from thousand.Player import Player\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from typing import Any\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallestPlayer(Player):\n",
    "\n",
    "    def make_a_move(self, observation, info):\n",
    "        return info['correct_moves'][0]\n",
    "\n",
    "\n",
    "class BiggestPlayer(Player):\n",
    "\n",
    "    def make_a_move(self, observation, info):\n",
    "        return info['correct_moves'][-1]\n",
    "\n",
    "\n",
    "class RandomPlayer(Player):\n",
    "\n",
    "    def __init__(self, seed=None) -> None:\n",
    "        self.rng: np.random.Generator = np.random.default_rng(seed)\n",
    "\n",
    "    def make_a_move(self, observation, info):\n",
    "        return self.rng.choice(info['correct_moves'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3iFr8ELzI490"
   },
   "outputs": [],
   "source": [
    "def get_action(env: gym.Env, network, observation, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    observation = torch.tensor(observation, dtype=torch.float32)\n",
    "    q_values = network(observation)\n",
    "    if np.random.random() < epsilon:\n",
    "        return int(np.random.choice(env.action_space.n))\n",
    "    return int(np.argmax(q_values.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AVFj3M10I491"
   },
   "outputs": [],
   "source": [
    "def compute_td_loss(network, states, actions, rewards, next_states, is_done, gamma=0.99):\n",
    "    \"\"\" Compute td loss using torch operations only. Use the formula above. \"\"\"\n",
    "    states = torch.tensor(\n",
    "        states, dtype=torch.float32)    # shape: [batch_size, state_size]\n",
    "    actions = torch.tensor(actions, dtype=torch.long)    # shape: [batch_size]\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32)  # shape: [batch_size]\n",
    "    # shape: [batch_size, state_size]\n",
    "    next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "    is_done = torch.tensor(is_done, dtype=torch.uint8)  # shape: [batch_size]\n",
    "\n",
    "    # get q-values for all actions in current states\n",
    "    predicted_qvalues = network(states)\n",
    "\n",
    "    # select q-values for chosen actions\n",
    "    predicted_qvalues_for_actions = predicted_qvalues[\n",
    "      range(states.shape[0]), actions\n",
    "    ]\n",
    "    # compute q-values for all actions in next states\n",
    "    predicted_next_qvalues = network(next_states)\n",
    "\n",
    "    # compute V*(next_states) using predicted next q-values\n",
    "    next_state_values = torch.amax(predicted_next_qvalues, axis=1)\n",
    "    assert next_state_values.dtype == torch.float32\n",
    "\n",
    "    # compute \"target q-values\" for loss - it's what's inside square parentheses in the above formula.\n",
    "    target_qvalues_for_actions = rewards + gamma * next_state_values\n",
    "\n",
    "    # at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "    target_qvalues_for_actions = torch.where(\n",
    "        is_done, rewards, target_qvalues_for_actions)\n",
    "\n",
    "    # mean squared error loss to minimize\n",
    "    loss = torch.mean((predicted_qvalues_for_actions -\n",
    "                       target_qvalues_for_actions.detach()) ** 2)\n",
    "\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eo6b0b4vI492"
   },
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EbtIGLEuI493"
   },
   "outputs": [],
   "source": [
    "def generate_session(env, options: dict[str, Any], network, optimizer, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    observation, info = env.reset(options=options)\n",
    "    is_done = False\n",
    "\n",
    "    while not is_done:    \n",
    "        action = get_action(env, network, observation, epsilon=epsilon)\n",
    "        next_observation, reward, terminated, truncated, info = env.step(action)\n",
    "        is_done = terminated or truncated\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            compute_td_loss(network, [observation], [action], [reward], [next_observation], [is_done]).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_reward += reward\n",
    "        observation = next_observation\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_game(env: gym.Env, options: dict[str, Any], network, optimizer, n_sessions: int, epsilon: float, n_games: int):\n",
    "    tr = trange(n_games, desc='mean: 0000; epsl: 0000')\n",
    "    means = []\n",
    "    for _ in tr:\n",
    "        rewards = [generate_session(env, options, network, optimizer, epsilon, train=True) for _ in range(n_sessions)]\n",
    "        epsilon *= 0.99\n",
    "        tr.set_description(\n",
    "            f'mean: {np.mean(rewards):4.1f}; epsl: {epsilon:4.3f}')\n",
    "        means.append(np.mean(rewards))\n",
    "\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_chart(mean, name):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.set_title('mean rewards')\n",
    "    ax.grid()\n",
    "    ax.plot(mean, label='mean')\n",
    "    ax.legend()\n",
    "    fig.savefig(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "G-79ol_HI493"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"Thousand-v1\")\n",
    "network = nn.Sequential(nn.Linear(env.observation_space.shape[0], 101), \n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(101, 101),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(101, 50),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(50, env.action_space.n))\n",
    "opt = torch.optim.Adam(network.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mean = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = learn_game(env, {'players': [SmallestPlayer(), SmallestPlayer()]}, network, opt, 100, 0.8, 500)\n",
    "all_mean += mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_chart(all_mean, f'{time.time()}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "task_12_approx_qlearning_easy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
